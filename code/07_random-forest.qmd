---
title: "07_randomforest"
format: html
editor: visual
---

# Setup

```{r}
#| message: false
#| warning: false

#install.packages("ranger")
library(beepr)
library(tidymodels)
library(tidyverse)
library(vip)
library(ranger)
library(finetune)
```

```{r fiber-env-data}
weather <- read_csv("../data/processed/training-data-ml-ready.csv")
weather
```

```{r}
summary(weather)
```

# ML workflow

We're going to use the same workflow as we used for elastic net.

## 1. Pre-processing

Here's where we perform **data split** and **data processing**.

### a. Data split

For data split, let's use **70% training / 30% testing**.

```{r weather_split}
# Setting seed to get reproducible results  
set.seed(931735)

# Setting split level  
weather_split <- initial_split(weather, 
                               prop = .7,
                               strata = yield_adj_mg_ha)

weather_split
```

```{r weather_train}
# Setting train set 
weather_train <- training(weather_split)

weather_train
```

How many observations?

```{r weather_test}
# Setting test split
weather_test <- testing(weather_split)

weather_test
```

How many observations?

Let's check the distribution of our predicted variable **strength_gtex** across training and testing:

```{r distribution}
ggplot() +
  geom_density(data = weather_train, 
               aes(x = yield_adj_mg_ha),
               color = "red") +
  geom_density(data = weather_test, 
               aes(x = yield_adj_mg_ha),
               color = "blue") 
  
```

Now, we put our **test set** aside and continue with our **train set** for training.

### b. Data processing

Before training, we need to perform some processing steps, like\
- normalizing\
- **removing unimportant variables**\
- dropping NAs\
- performing PCA on the go\
- removing columns with single value\
- others?

For that, we'll create a **recipe** of these processing steps.

This recipe will then be applied now to the **train data**, and easily applied to the **test data** when we bring it back at the end.

Creating a recipe is an easy way to port your processing steps for other data sets without needing to repeat code, and also only considering the data it is being applied to.

You can find all available recipe step options here: https://tidymodels.github.io/recipes/reference/index.html

Different model types require different processing steps.\
Let's check what steps are required for an elastic net model (linear_reg). We can search for that in this link: https://www.tmwr.org/pre-proc-table

> Differently from elastic net, variables do not need to be normalized in random forest, so we'll skip this step.

```{r weather_recipe}
weather_recipe <-
  # Defining predicted and predictor variables
  recipe(yield_adj_mg_ha ~ .,
         data = weather_train) %>%
  # Removing year and site  
    step_rm( 
            year,       # Remove year identifier
    site,       # Remove site identifier
    hybrid, # Remove hybrid; non-numeric
    days_to_harvest, # Remove days from plant to harvest
    contains("gdd"),
    grain_moisture) # Remove grain moisture; not predictive #%>%
  # Normalizing all numeric variables except predicted variable
  #step_normalize(all_numeric(), -all_outcomes())

weather_recipe
# Create recipe for data preprocessing

```

Now that we have our recipe ready, we **need to apply it** to the training data in a process called prepping:

```{r weather_prep}
weather_prep <- weather_recipe %>%
  prep()

weather_prep
```

Now, we're ready to start the model training process!

## 2. Training

### a. Model specification

First, let's specify:\
- the **type of model** we want to train\
- which **engine** we want to use\
- which **mode** we want to use

> Elastic nets can only be run for a numerical response variable. Random forests can be run with either numerical (regression) or categorical (classification) explanatory variable. Therefore, we have the need to specify the mode here.

Random forest **hyperparameters**:\
- **trees**: number of trees in the forest\
- **mtry**: number of random features sampled at each node split\
- **min_n**: minimum number of data points in a node that are required for the node to be split further

Let's create a model specification that will **fine-tune** the first two for us.

A given model type can be fit with different engines (e.g., through different packages). Here, we'll use the **ranger** engine/package.

```{r rf_spec}
rf_spec <- 
  # Specifying rf as our model type, asking to tune the hyperparameters
rand_forest(trees = tune(),
            mtry = tune()
            ) %>%
    # Specify the engine
set_engine("ranger") %>%
    # Specifying mode  
set_mode("regression")

rf_spec
```

Notice how the main arguments above do not have a value **yet**, because they will be tuned.

### b. Hyper-parameter tuning

For our iterative search, we need:\
- Our model specification (`rf_spec`)\
- The recipe (`weather_recipe`)\
- Our **resampling strategy** (don't have yet)

> Notice that for rf we do not need to specify the parameter information, as we needed for CIT. The reason is that for rf, all hyperparameters to be tuned are specified at the model level, whereas for CIT one was at model level and one was at the engine level. Engine level hyperparameters need to be "finalized" and have their range of values set up before they can be used in search methods.

> We used 10-fold CV before. It took about 10-min to run the grid on my side, so to avoid a long wait time in class, let's switch to 5-fold CV this time around.

Let's define our resampling strategy below, using a 5-fold cross validation approach:

```{r resampling_foldcv}
set.seed(34549)
resampling_foldcv <- vfold_cv(weather_train, 
                              v = 10)

resampling_foldcv
resampling_foldcv$splits[[1]]
resampling_foldcv$splits[[2]]
```

On each fold, we'll use **389** observations for training and **98** observations to assess performance.

Now, let's perform the search below.

We will use an iterative search algorithm called **simulated annealing**.

Here's how it works:\
![](https://www.tmwr.org/figures/iterative-neighborhood-1.png) - In the example above, mixture and penalty from an elastic net model are being tuned.

-   It finds a candidate value of hyperparameters and their associated rmse to start (iteration 1).

-   It establishes a radius around the first proposal, and randomly chooses a new set of values within that radius.

-   If this achieves better results than the previous parameters, it is accepted as the new best and the process continues. If the results are worse than the previous value the search procedure may still use this parameter to define further steps.

-   After a given number of iterations, the algorithm stops and provides a list of the best models and their hyperparameters.

In the algorithm below, we are asking for 50 iterations.

```{r rf_grid_result}
set.seed(76544)
rf_grid_result <- tune_sim_anneal(object = rf_spec,
                     preprocessor = weather_recipe,
                     resamples = resampling_foldcv,
                    #param_info = rf_param,
                    iter = 15
                     )

beepr::beep()

rf_grid_result
rf_grid_result$.metrics[[2]]
```

Notice how we have a column for iterations.\
The first iteration uses a sensible value for the hyper-parameters, and then starts "walking" the parameter space in the direction of greatest improvement.

Let's collect a summary of metrics (across all folds, for each iteration), and plot them.

Firs, RMSE (lower is better):

```{r RMSE}
rf_grid_result %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  ggplot(aes(x = mtry, 
             y = trees 
             )) +
  geom_path(group = 1) +
  geom_point(aes(color = mean),
             size = 3) + 
  scale_color_viridis_b() +
  geom_text(aes(label = .iter), nudge_x = .0005) +
  labs(title = "RMSE")
```

What tree_depth and min criterion values created lowest RMSE?

Now, let's look into R2 (higher is better):

```{r R2}
rf_grid_result %>%
  collect_metrics() %>%
  filter(.metric == "rsq") %>%
  ggplot(aes(x = mtry, 
             y = trees 
             )) +
  geom_path(group = 1) +
  geom_point(aes(color = mean),
             size = 3) + 
  scale_color_viridis_b() +
  geom_text(aes(label = .iter), nudge_x = .0005) +
  labs(title = "R2")

```

> Previously, we selected the single best model. Now, let's select the best model within one std error of the metric, so we choose a model among the top ones that is more parsimonious.

```{r}
# Based on lowest RMSE
best_rmse <- rf_grid_result %>%
  select_by_pct_loss("trees",
                     metric = "rmse",
                     limit = 2)

best_rmse

```

```{r}
# Based on greatest R2
best_r2 <- rf_grid_result %>%
  select_by_pct_loss("trees",
                     metric = "rsq",
                     limit = 2)


best_r2

```


Let's use the hyperparameter values that optimized R2 to fit our final model.

```{r final_spec}
final_spec <- rand_forest(trees = 365,
                          mtry = 59) %>%
  # Specify the engine
set_engine("ranger",
           importance = "permutation") %>%
    # Specifying mode  
  set_mode("regression")
  

final_spec
```

## 3. Validation

Now that we determined our best model, let's do our **last fit**.

This means 2 things:\
- Traninig the optimum hyperparameter values on the **entire training set**\
- Using it to **predict** on the **test set**

These 2 steps can be completed in one function, as below:

```{r final_fit}
final_fit <- last_fit(final_spec,
                weather_recipe,
                split = weather_split)

final_fit %>%
  collect_predictions()
```

Metrics on the **test set**:

```{r}
final_fit %>%
  collect_metrics()
```

Metrics on **train set** (for curiosity and compare to test set):

```{r}
# RMSE
final_spec %>%
  fit(yield_adj_mg_ha ~ .,
      data = bake(weather_prep, 
                  weather_train)) %>%
  augment(new_data = bake(weather_prep, 
                          weather_train)) %>% 
  rmse(yield_adj_mg_ha, .pred) %>%
  bind_rows(
    
    
    # R2
    final_spec %>%
      fit(yield_adj_mg_ha ~ .,
          data = bake(weather_prep, 
                      weather_train)) %>%
      augment(new_data = bake(weather_prep, 
                              weather_train)) %>% 
      rsq(yield_adj_mg_ha, .pred)
    
  )

```

How does metrics on test compare to metrics on train?

Predicted vs. observed plot:

```{r}
final_fit %>%
  collect_predictions() %>%
  ggplot(aes(x = yield_adj_mg_ha,
             y = .pred)) +
  geom_point() +
  geom_abline() +
  geom_smooth(method = "lm") +
  annotate("text", x = -Inf, y = Inf, 
           label = "R² = 0.55\nRMSE = 1.89", 
           hjust = -0.1, vjust = 1.1, size = 4, fontface = "bold") +
  labs(
    x = "Actual Yield (mg/ha)",
    y = "Predicted Yield (mg/ha)",
    title = "Predicted vs. Actual Yield"
  )

ggsave(filename = file.path("../output", "rf_pred_v_obs.png"), 
       width = 8, 
       height = 6, 
       dpi = 300)
```

Variable importance:

The importance metric we are evaluating here is **permutation**.

In the permutation-based approach, for each tree, the out- of-bag sample is passed down the tree and the prediction accuracy is recorded.

Then the values for each variable (one at a time) are randomly permuted and the accuracy is again computed.

The decrease in accuracy as a result of this randomly shuffling of feature values is averaged over all the trees for each predictor.

The variables with the **largest average decrease in accuracy** are considered **most important**.
```{r}
# Extract variable importance
vi_df<- final_spec %>%
  fit(yield_adj_mg_ha ~ ., data = bake(weather_prep, weather)) %>%
  vi() %>%
  mutate(
    Variable = fct_reorder(Variable, 
                           Importance))

# Save variable importance data
write_csv(vi_df, "../output/rf_variable_importance.csv")
```

```{r}
vi_plot <- vi_df %>%
  mutate(Variable = fct_reorder(Variable, Importance)) %>%
  slice_max(Importance, n = 15) 
  ggplot(aes(x = Importance, y = Variable)) +
  geom_col(aes(fill = Importance), width = 0.7, show.legend = FALSE) +
  scale_fill_viridis_c(option = "D", direction = -1) +
  labs(
    title = "Variable Importance - (Random Forest)",
    x = "Permutation Importance",
    y = NULL
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold"),
    axis.text.y = element_text(face = "bold"),
    panel.grid.major.y = element_blank()
  )

# Save high-resolution PNG
ggsave("../output/rf_variable_importance_full.png", vi_plot,
       width = 8, height = 6, dpi = 600, bg = "white")

```


```{r}
final_spec %>%
  fit(yield_adj_mg_ha ~ .,
         data = bake(weather_prep, weather)) %>%
    vi() %>%
  mutate(
    Variable = fct_reorder(Variable, 
                           Importance)
  ) %>%
  ggplot(aes(x = Importance, 
             y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)

ggsave(filename = file.path("../output", "rf_imp-variable.png"), 
       width = 8, 
       height = 6, 
       dpi = 300)
    
```

```{r}
# Load libraries
library(ggplot2)
library(vip)
library(tibble)
library(dplyr)
library(tidymodels)
library(forcats)

# 1. Predicted vs Observed on TEST set
final_fit %>%
  collect_predictions() %>%
  ggplot(aes(x = str_g_per_tex, y = .pred)) +
  geom_point(alpha = 0.8, size = 2, color = "darkblue") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray50") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(
    x = "Observed Strength (g/tex)",
    y = "Predicted Strength (g/tex)",
    title = "Observed vs Predicted (Test Set)"
  ) +
  theme_minimal()

# 2. Variable importance
vi_df <- final_spec %>%
  fit(str_g_per_tex ~ ., data = bake(weather_prep, weather_train)) %>%
  vi() %>%
  mutate(Variable = fct_reorder(Variable, Importance))

vi_df <- ggplot(vi_df, aes(x = Importance, y = Variable)) +
  geom_col(aes(fill = Importance), width = 0.7, show.legend = FALSE) +
  scale_fill_viridis_c(option = "D", direction = -1) +
  labs(
    title = "Top 15 Most Important Variables",
    x = "Importance Score",
    y = NULL
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold"),
    axis.text.y = element_text(face = "bold"),
    panel.grid.major.y = element_blank()
  )
# Save high-res PNG
ggsave("../output/04_22_vi_rf_strength_top15.png",
       vi_df,
       width = 10, height = 6, dpi = 400, bg = "white")
# 3. Save predictions and variable importance (optional)
test_preds <- final_fit %>% collect_predictions()
train_preds <- final_spec %>%
  fit(str_g_per_tex ~ ., data = bake(weather_prep, weather_train)) %>%
  augment(new_data = bake(weather_prep, weather_train))

write_csv(test_preds, "../output/rf_strength_test_predictions.csv")
write_csv(train_preds, "../output/rf_strength_train_predictions.csv")
write_csv(vi_df, "../output/rf_strength_variable_importance.csv")

# Save model and recipe
saveRDS(final_spec, file = "../output/final_rf_strength_model.rds")
saveRDS(weather_prep, file = "../output/final_rf_strength_weather_recipe_prep.rds")

```



```{r fit model to test}

# Load the test data sheet to run predictions on
prediction_data <- read_csv("../data/processed/test-data-cleaned.csv")

prediction_data
```

```{r}
# Create a workflow bundling the model specification and recipe
workflow_fit <- workflow() %>%
  add_model(final_spec) %>%
  add_recipe(weather_recipe)
```

```{r}

# Fit the workflow to your training data
fitted_workflow <- fit(workflow_fit, data = weather)
```

```{r}
# Make predictions using the *fitted* workflow
predictions <- predict(fitted_workflow, prediction_data)
```

```{r}
# Add the predictions as a new column to the new data
prediction_data %>% 
  mutate(yield = predictions) %>%
  unnest() %>%
  mutate(yield = .pred) %>%
  dplyr::select(-.pred) %>%
  dplyr::select(yield, everything()) %>%
  dplyr::select(hybrid, everything()) %>%
  dplyr::select(site, everything()) %>%
  dplyr::select(year, everything()) %>%
  write_csv("../data/processed/predictions.csv")
```


```{r}
predictions <-
  read_csv("../data/processed/predictions.csv")

glimpse(predictions)
```

```{r}
submission <- read.csv("../data/raw/testing_submission.csv")

submission

```

```{r}
pred_clean <- predictions %>%
  dplyr::select(year, site, hybrid, yield) %>%
  rename(yield_mg_ha = yield) 
  
```

```{r}
pred_clean %>%
  left_join(submission)
unique(pred_clean$year)
```

```{r}
summary(pred_clean)
```
```{r}
pred_clean %>%
  write_csv("../data/processed/testing-submission-rf.csv")
```

